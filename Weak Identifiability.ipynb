{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a4328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.special\n",
    "inf  = float(\"inf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82a0386e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa15dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Env(object):\n",
    "    def __init__(self, n_states, n_actions, horizon, discount = 0.99, unique_start = True, deterministic = False):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions= n_actions\n",
    "        self.horizon = horizon\n",
    "        self.discount = discount\n",
    "        self.unique_start = unique_start\n",
    "        self.deterministic = deterministic\n",
    "        \n",
    "        if self.unique_start:\n",
    "            self.start_state = np.random.choice(np.arange(self.n_states))\n",
    "            self.start_distribution = np.zeros(self.n_states)\n",
    "            self.start_distribution[self.start_state] = 1\n",
    "            \n",
    "        else:\n",
    "            self.start_distribution = np.random.rand(num_states)\n",
    "            self.start_distribution /= np.sum(self.start_distribution)\n",
    "#         self.initial_state_distribution = (1/self.n_states)*np.ones(self.n_states)\n",
    "        self.mdp_transition_matrices = []\n",
    "        \n",
    "        self.get_random_transition_matrices()\n",
    "        \n",
    "    def get_random_transition_matrices(self):\n",
    "        k = self.n_states\n",
    "        T = np.full(shape = (self.n_states*self.n_actions, self.n_states) , fill_value=0)\n",
    "        T = np.zeros((self.n_states*self.n_actions, self.n_states))\n",
    "        \n",
    "        for i in range(self.n_actions):\n",
    "            \n",
    "            Pa = np.identity(k) + \\\n",
    "                    np.random.uniform(low=0., high=.25, size=(k, k))\n",
    "            Pa /= Pa.sum(axis=1, keepdims=1)\n",
    "            \n",
    "            if self.deterministic:\n",
    "                Pa = np.random.permutation(np.eye(self.n_states))\n",
    "                \n",
    "            self.mdp_transition_matrices.append(Pa)\n",
    "            T[i*k:(i+1)*k,:] = Pa\n",
    "        \n",
    "        assert (np.linalg.norm(T.sum(axis = 1)- np.ones(T.shape[0]))) <= 1e-5\n",
    "        \n",
    "        self.transition_matrix = T\n",
    "    \n",
    "    def generate_trajectories(self):\n",
    "        # Memoization dictionary to store already computed results\n",
    "        horizon = self.horizon\n",
    "        memo_dict = {}\n",
    "\n",
    "        def generate_recursive(current_time, current_state):\n",
    "            # Check if the result is already computed\n",
    "            if (current_time, current_state) in memo_dict:\n",
    "                return memo_dict[(current_time, current_state)]\n",
    "\n",
    "            # Base case: when we reach the horizon, return an empty trajectory\n",
    "            if current_time == horizon:\n",
    "                return [[]]\n",
    "\n",
    "            trajectories = [] \n",
    "\n",
    "            # Recursive case: try all actions and next states\n",
    "            for action in range(self.n_actions):\n",
    "                next_state_probs = self.mdp_transition_matrices[action][current_state] if current_state is not None else self.start_distribution\n",
    "                \n",
    "                for next_state in range(self.n_states):\n",
    "                    # Recursive call\n",
    "                    if next_state_probs[next_state] != 0:\n",
    "                        next_trajectories = generate_recursive(current_time + 1, next_state)\n",
    "\n",
    "                        # Extend the current trajectory with the current action and next state\n",
    "                        if current_state == None:\n",
    "                            \n",
    "                            trajectories.extend([(next_state, action )] + traj for traj in next_trajectories)\n",
    "                        else:\n",
    "                            trajectories.extend([(current_state, action, next_state)] + traj for traj in next_trajectories)\n",
    "\n",
    "            # Memoize the result before returning\n",
    "            memo_dict[(current_time, current_state)] = trajectories\n",
    "            return trajectories\n",
    "\n",
    "        # Start the recursion with time = 0 and no initial state\n",
    "        # For a unique starting state\n",
    "        return generate_recursive(0, self.start_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618eaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232b49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813e674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "ed41c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def soft_bellman_operation(env, reward):\n",
    "    \n",
    "    # Input:    env    :  environment object\n",
    "    #           reward :  SxA dimensional vector \n",
    "    #           horizon:  finite horizon\n",
    "        \n",
    "    discount = env.discount\n",
    "    horizon = env.horizon\n",
    "    \n",
    "    if horizon is None or math.isinf(horizon):\n",
    "        raise ValueError(\"Only finite horizon environments are supported\")\n",
    "    \n",
    "    n_states  = env.n_states\n",
    "    n_actions = env.n_actions\n",
    "    \n",
    "    T = env.transition_matrix\n",
    "    \n",
    "    V = np.zeros(shape = (horizon, n_states))\n",
    "    Q = np.zeros(shape = (horizon, n_states,n_actions))\n",
    "        \n",
    "    broad_R = reward\n",
    "\n",
    "    # Base case: final timestep\n",
    "    # final Q(s,a) is just reward\n",
    "    Q[horizon - 1, :, :] = broad_R\n",
    "    # V(s) is always normalising constant\n",
    "    V[horizon - 1, :] = scipy.special.logsumexp(Q[horizon - 1, :, :], axis=1)\n",
    "\n",
    "    # Recursive case\n",
    "    for t in reversed(range(horizon - 1)):\n",
    "#         next_values_s_a = T @ V[t + 1, :]\n",
    "#         next_values_s_a = next_values_s_a.reshape(n_states,n_actions)\n",
    "        for a in range(n_actions):\n",
    "            Ta = env.mdp_transition_matrices[a]\n",
    "            next_values_s_a = Ta@V[t + 1, :]\n",
    "            Q[t, :, a] = broad_R[:,a] + discount * next_values_s_a\n",
    "            \n",
    "        V[t, :] = scipy.special.logsumexp(Q[t, :, :], axis=1)\n",
    "\n",
    "    pi = np.exp(Q - V[:, :, None])\n",
    "\n",
    "    return V, Q, pi\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "fdb07497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.57245616e-05, 9.99984275e-01],\n",
       "        [5.21088218e-35, 1.00000000e+00]],\n",
       "\n",
       "       [[1.82596478e-39, 1.00000000e+00],\n",
       "        [3.09749671e-01, 6.90250329e-01]],\n",
       "\n",
       "       [[5.01599933e-05, 9.99949840e-01],\n",
       "        [1.63349336e-35, 1.00000000e+00]],\n",
       "\n",
       "       [[2.00847295e-43, 1.00000000e+00],\n",
       "        [9.99754945e-01, 2.45054634e-04]],\n",
       "\n",
       "       [[5.00000000e-01, 5.00000000e-01],\n",
       "        [8.19401262e-40, 1.00000000e+00]]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaaff10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2ab58e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env.start_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aabd43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "d419114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectory_prob(traj, policy, env):\n",
    "    # the policy is of shape (horizon, n_states, n_actions)\n",
    "    p = 1.0\n",
    "    \n",
    "    for t, (s,a,s_prime) in enumerate(traj):\n",
    "        \n",
    "        if t == 0:\n",
    "            p *= env.start_distribution[s]\n",
    "        \n",
    "        # policy probability\n",
    "        p_policy = policy[t][s][a]\n",
    "        # tranisition porbability\n",
    "        p_trans = env.mdp_transition_matrices[a][s][s_prime]\n",
    "        \n",
    "        p*= (p_policy*p_trans) \n",
    "        \n",
    "    \n",
    "    \n",
    "    return p\n",
    "\n",
    "def compute_trajectory_probability(traj, pi,env):\n",
    "    P = []\n",
    "\n",
    "    \n",
    "    for t in traj:\n",
    "        p = compute_trajectory_prob(t,pi, env)\n",
    "        P.append(p)\n",
    "        \n",
    "    # assert that we indeed have a distribution\n",
    "    assert np.abs(1 - sum(P)) < 1e-4\n",
    "    \n",
    "    return P\n",
    "\n",
    "\n",
    "def discounted_reward_sum(reward,trajectory,env):\n",
    "    gamma = env.discount\n",
    "#     gamma = 1\n",
    "    discounted_reward = 0\n",
    "    for t, (s,a,sprime) in enumerate(trajectory):\n",
    "        discounted_reward += (gamma**t)*reward[s][a]\n",
    "        \n",
    "    return discounted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "c1c20a8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08993390049290516\n",
      "False\n",
      "0.07626355140768996\n",
      "False\n",
      "0.05310474439222251\n",
      "False\n",
      "0.03497576638040493\n",
      "True\n",
      "0.036512315481554924\n",
      "True\n",
      "0.04277940124418122\n",
      "True\n",
      "0.10189595469664141\n",
      "False\n",
      "0.04937138482912686\n",
      "True\n",
      "0.06121017149002214\n",
      "False\n",
      "0.053511098479151684\n",
      "False\n",
      "0.05185524697904988\n",
      "False\n",
      "0.0289012121031744\n",
      "True\n",
      "0.10463514377189267\n",
      "False\n",
      "0.11670643981956438\n",
      "False\n",
      "0.1353595210320439\n",
      "False\n",
      "0.04852164481848946\n",
      "True\n",
      "0.17672186490187633\n",
      "False\n",
      "0.06293489198513877\n",
      "False\n",
      "0.10992820571266598\n",
      "False\n",
      "0.11612034834533148\n",
      "False\n",
      "0.09588416677706083\n",
      "False\n",
      "0.02946626933267505\n",
      "True\n",
      "0.2730231529825999\n",
      "False\n",
      "0.08932339837048639\n",
      "False\n",
      "0.06730740108784096\n",
      "False\n",
      "0.027331806733669316\n",
      "True\n",
      "0.02491748798198157\n",
      "True\n",
      "0.04602119225205987\n",
      "True\n",
      "0.13371270116148723\n",
      "False\n",
      "0.05079035858006201\n",
      "False\n",
      "0.026600797510295492\n",
      "True\n",
      "0.05362654348557237\n",
      "False\n",
      "0.04217651823971933\n",
      "True\n",
      "0.09232144837591005\n",
      "False\n",
      "0.17050981974131615\n",
      "False\n",
      "0.08941998445095013\n",
      "False\n",
      "0.10771125071785499\n",
      "False\n",
      "0.08343184977594073\n",
      "False\n",
      "0.06861176335879363\n",
      "False\n",
      "0.04843733171948948\n",
      "True\n",
      "0.08492693252285366\n",
      "False\n",
      "0.028486918245403284\n",
      "True\n",
      "0.039524879161960846\n",
      "True\n",
      "0.06896998466576532\n",
      "False\n",
      "0.04638634775414418\n",
      "True\n",
      "0.06938607353280889\n",
      "False\n",
      "0.04033226870764487\n",
      "True\n",
      "0.10997198226240984\n",
      "False\n",
      "0.05589516995054593\n",
      "False\n",
      "0.06293898346607037\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    n_states = 3\n",
    "    n_actions = 3\n",
    "    horizon = 5\n",
    "\n",
    "    env = Env(n_states,n_actions,horizon, deterministic = False)\n",
    "    reward = np.zeros(shape = (n_states,n_actions))\n",
    "    reward[-1,:] = 10\n",
    "    reward[-1,-1] = 100\n",
    "    reward = np.random.randn(env.n_states, env.n_actions)\n",
    "    V, Q, pi = soft_bellman_operation(env,reward )\n",
    "    traj = env.generate_trajectories()\n",
    "    l, P, Pz = confirm_boltzman_distribution(reward, traj, env)\n",
    "#     if not l:\n",
    "    print(np.linalg.norm(P-Pz))\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "4d97231b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9765625"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a85f4dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [10., 10., 10.]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "b1a57cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirm_boltzman_distribution(reward, traj, env):\n",
    "    V_1, Q_1, pi = soft_bellman_operation(env,reward)\n",
    "    \n",
    "    P = compute_trajectory_probability(traj, pi,env)\n",
    " \n",
    "    P_ziebart = []\n",
    "    for t in traj:\n",
    "        rr = discounted_reward_sum(reward,t,env)\n",
    "\n",
    "        P_ziebart.append(rr)\n",
    "    \n",
    "    Z = scipy.special.logsumexp(P_ziebart)\n",
    "    P_z = np.exp(P_ziebart - Z)\n",
    "#     print(P_z)\n",
    "    return True if np.linalg.norm(P- P_z) <= 5e-2 else False, P,P_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ccf9147f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.00000000e+00, 1.01122149e-43, 1.01122149e-43],\n",
       "        [1.01122149e-43, 1.01122149e-43, 1.00000000e+00],\n",
       "        [8.28596168e-83, 8.28596168e-83, 1.00000000e+00]],\n",
       "\n",
       "       [[1.00000000e+00, 1.01122149e-43, 1.01122149e-43],\n",
       "        [1.01122149e-43, 1.01122149e-43, 1.00000000e+00],\n",
       "        [8.28596168e-83, 8.28596168e-83, 1.00000000e+00]],\n",
       "\n",
       "       [[1.00000000e+00, 3.00051867e-43, 3.00051867e-43],\n",
       "        [3.00051867e-43, 3.00051867e-43, 1.00000000e+00],\n",
       "        [2.45862879e-82, 2.45862879e-82, 1.00000000e+00]],\n",
       "\n",
       "       [[3.33333333e-01, 3.33333333e-01, 3.33333333e-01],\n",
       "        [3.33333333e-01, 3.33333333e-01, 3.33333333e-01],\n",
       "        [8.19401262e-40, 8.19401262e-40, 1.00000000e+00]]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "39b21d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "n_states = 3\n",
    "n_actions = 3\n",
    "horizon = 4\n",
    "\n",
    "env = Env(n_states,n_actions,horizon, deterministic = True)\n",
    "reward = np.zeros(shape = (n_states,n_actions))\n",
    "reward[-1,:] = 10\n",
    "reward[-1,-1] = 100\n",
    "V, Q, pi = soft_bellman_operation(env,reward )\n",
    "traj = env.generate_trajectories()\n",
    "l, P, Pz = confirm_boltzman_distribution(reward, traj, env)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "22769136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(P).round(decimals = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "49df0526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(Pz).round(decimals = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "6403d426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24982517591401415"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "35f9ad89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24979147677944472"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(Pz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "02bdaa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5723bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def check_reward_equivalence(r_1,r_2,T):\n",
    "    # check weak identifiability equivalence\n",
    "    diff = []\n",
    "    for trajectory in T:\n",
    "        discounted_r_1 = discounted_reward_sum(r_1,trajectory, env)\n",
    "        discounted_r_2 = discounted_reward_sum(r_2,trajectory, env)\n",
    "        \n",
    "        c = discounted_r_1 - discounted_r_2\n",
    "        diff.append(c)\n",
    "        \n",
    "    diff = np.array(diff)\n",
    "    # need to round for floating-point precision\n",
    "    \n",
    "#     print(\"Unique in d: \",np.unique(diff.round(decimals=4)))\n",
    "\n",
    "    if np.unique(diff.round(decimals=4)).shape[0] == 1:\n",
    "#         print(\"Rewards are equivalent\")\n",
    "        return True\n",
    "    else:\n",
    "#         print(\"Rewards are not equivalent\")\n",
    "        return False\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ed3c7",
   "metadata": {},
   "source": [
    "# Unit Test for the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4123f5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique in d:  [-679.3465]\n",
      "Rewards are equivalent\n"
     ]
    }
   ],
   "source": [
    "reward_2 = reward.copy() + 100\n",
    "# reward_2[0] = 0\n",
    "# reward_2 = np.zeros((2,2))\n",
    "d = check_reward_equivalence(reward,reward_2,traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d4c04",
   "metadata": {},
   "source": [
    "# Code to see if two probability distributions are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cc305907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_distribution_equivalence(reward_1, reward_2, env, trajectories):\n",
    "    \n",
    "    V_1, Q_1, pi_1 = soft_bellman_operation(env,reward_1)\n",
    "    V_2, Q_2, pi_2 = soft_bellman_operation(env,reward_2)\n",
    "    \n",
    "    P1 = compute_trajectory_probability(trajectories,pi_1,env)\n",
    "    P2 = compute_trajectory_probability(trajectories,pi_2,env)\n",
    "    \n",
    "    return True if np.linalg.norm(np.array(P1)-np.array(P2)) <= 1e-5 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "382ca614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_distribution_equivalence(reward, reward_2, env, traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca893d",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8fbf7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weak_identifiability(reward_1, reward_2, env, trajectories):\n",
    "    V_1, Q_1, pi_1 = soft_bellman_operation(env,reward_1)\n",
    "    V_2, Q_2, pi_2 = soft_bellman_operation(env,reward_2)\n",
    "    \n",
    "    P1 = compute_trajectory_probability(trajectories,pi_1,env)\n",
    "    P2 = compute_trajectory_probability(trajectories,pi_2,env)\n",
    "    \n",
    "    # first check if the distributions are unqiue:\n",
    "    first_branch = True if np.linalg.norm(np.array(P1)-np.array(P2)) <= 1e-5 else False\n",
    "    \n",
    "    # if first_branch == False, then we are on the first left branch\n",
    "    if first_branch == False:\n",
    "        test_1 = check_reward_equivalence(reward_1,reward_2,trajectories)\n",
    "        if test_1:\n",
    "            print(\"Not weakly identifiable\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Weakly Identifabiable\")\n",
    "            return\n",
    "    \n",
    "    # first_branch == True, so since we reach here, then we are on the right side of the tree\n",
    "    # check reward equivalence for the equal trajectories\n",
    "    test_1 = check_reward_equivalence(reward_1,reward_2,trajectories)\n",
    "    \n",
    "    if test_1 == False:\n",
    "        print(\"Not weakly identifiable\")\n",
    "        return\n",
    "    \n",
    "    # now we need to check proper model:\n",
    "    if test_1 and first_branch:\n",
    "        print(\"weakly identifiable\")\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d1cb8c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weakly Identifabiable\n"
     ]
    }
   ],
   "source": [
    "reward_1 = reward.copy()\n",
    "reward_2 = reward.copy() + 5\n",
    "reward_2[0] = 0\n",
    "reward_2[0][1] = 1\n",
    "check_weak_identifiability(reward, reward_2, env, traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "886b78b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [15., 15.]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1267f513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [10., 10.]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "c8222d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_distribution_equivalence(reward_1, reward_2, env, traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d0d03d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(np.eye(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aaffe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018a92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6710e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541ba00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
